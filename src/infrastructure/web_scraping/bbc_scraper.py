import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Optional
import re
from urllib.parse import urljoin, quote

from ...domain.entities.news import News
from ...domain.entities.news_source import NewsSource, SourceType


class BBCScraper:
    """Scraper espec√≠fico para BBC News."""
    
    def __init__(self):
        self.base_url = "https://www.bbc.com"
        self.search_url = "https://www.bbc.com/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def get_news_source(self) -> NewsSource:
        """Retorna a configura√ß√£o da fonte BBC News."""
        
        return NewsSource(
            name="BBC News",
            base_url=self.base_url,
            source_type=SourceType.WEB_SCRAPING,
            search_endpoint="/search",
            config={
                "max_results": 20,
                "timeout": 30,
                "retry_attempts": 3
            }
        )
    
    async def search_adam_sandler_news(self, session: aiohttp.ClientSession, 
                                     max_results: int = 10) -> List[News]:
        """Busca not√≠cias sobre Adam Sandler na BBC."""
        
        query = "Adam Sandler"
        
        try:
            url = f"{self.search_url}?q={quote(query)}"
            print(f"URL de busca: {url}")
            
            async with session.get(url, headers=self.headers, timeout=30) as response:
                
                if response.status != 200:
                    print(f"Erro ao acessar BBC: Status {response.status}")
                    return []
                
                html = await response.text()
                print(f"HTML da busca: {html[:500]}...")  # Log parte do HTML
                return await self._parse_search_results(html, max_results, session)
                
        except asyncio.TimeoutError:
            print("Timeout ao acessar BBC News")
            return []
        except Exception as e:
            print(f"Erro ao buscar not√≠cias na BBC: {str(e)}")
            return []
    
    async def _parse_search_results(self, html: str, max_results: int, session: aiohttp.ClientSession) -> List[News]:
        """Analisa os resultados de busca da BBC."""
        soup = BeautifulSoup(html, 'html.parser')
        news_list = []
        
        # BBC usa diferentes seletores dependendo da vers√£o da p√°gina
        # Vamos tentar m√∫ltiplos seletores
        selectors = [
            '[data-testid="newport-card"]',
            'div[data-testid="search-results"] article',
            '.ssrcss-1f3bvyz-Stack',
            'article',
            '.media__content'
        ]
        
        articles = []
        for selector in selectors:
            articles = soup.select(selector)
            print(f"Tentando seletor '{selector}': encontrados {len(articles)} elementos")
            if articles:
                print(f"Usando seletor '{selector}' - encontrados {len(articles)} artigos")
                break
        
        if not articles:
            print("Nenhum artigo encontrado com os seletores principais")
            # Fallback: buscar por links que contenham palavras-chave
            links = soup.find_all('a', href=True)
            print(f"Tentando fallback com links: encontrados {len(links)} links")
            for link in links[:max_results]:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if self._is_relevant_link(text, href):
                    news = self._create_news_from_link(link)
                    if news:
                        # Tentar buscar conte√∫do completo
                        full_content = await self.get_article_content(session, news.url)
                        if full_content:
                            news.content = full_content
                        news_list.append(news)
            
            print(f"Fallback retornou {len(news_list)} not√≠cias")
            return news_list[:max_results]
        
        for article in articles[:max_results]:
            try:
                news = self._extract_news_from_article(article)
                if news:
                    print(f"Not√≠cia extra√≠da: '{news.title}' - URL: {news.url}")
                    
                    # Buscar conte√∫do completo do artigo
                    print(f"üîç Buscando conte√∫do completo de: {news.url}")
                    full_content = await self.get_article_content(session, news.url)
                    if full_content and len(full_content) > len(news.content):
                        print(f"‚úÖ Conte√∫do completo encontrado ({len(full_content)} caracteres)")
                        news.content = full_content
                    else:
                        print(f"‚ö†Ô∏è Usando conte√∫do b√°sico ({len(news.content)} caracteres)")
                    
                    if news.is_relevant_to_adam_sandler():
                        print(f"‚úÖ Not√≠cia relevante: {news.title}")
                        news_list.append(news)
                    else:
                        print(f"‚ùå Not√≠cia n√£o relevante: {news.title}")
                else:
                    print("‚ùå Falha ao extrair not√≠cia do artigo")
            except Exception as e:
                print(f"Erro ao extrair not√≠cia: {str(e)}")
                continue
        
        return news_list[:max_results]
    
    def _extract_news_from_article(self, article) -> Optional[News]:
        """Extrai informa√ß√µes de uma not√≠cia de um elemento article."""
        try:
            # Tentar diferentes seletores para t√≠tulo
            title_selectors = ['h3', 'h2', 'h1', '.media__title', '[data-testid="card-headline"]']
            title = ""
            
            for selector in title_selectors:
                title_elem = article.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            if not title:
                return None
            
            # Tentar encontrar link
            link_elem = article.find('a', href=True)
            if not link_elem:
                return None
            
            url = link_elem.get('href', '')
            if url.startswith('/'):
                url = urljoin(self.base_url, url)
            
            # Tentar encontrar resumo/conte√∫do b√°sico primeiro
            content_selectors = [
                '[data-component="text-block"]',
                '[data-testid="card-description"]',
                '.media__summary', 
                'p', 
                '.ssrcss-1q0x1qg-Paragraph',
                '[data-testid="card-text"]'
            ]
            content = ""
            
            for selector in content_selectors:
                content_elem = article.select_one(selector)
                
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    if content and len(content) > 10:  # S√≥ aceitar se tiver conte√∫do substancial
                        break
            
            if not content or len(content) <= 10:
                content = title  # Usar t√≠tulo como conte√∫do se n√£o encontrar resumo
            
            # Data (dif√≠cil de extrair da BBC, usar data atual)
            published_date = datetime.now()
            
            # Criar not√≠cia inicial
            news = News(
                title=title,
                content=content,
                url=url,
                source="BBC News",
                published_date=published_date,
                author=None
            )
            
            return news
            
        except Exception as e:
            print(f"Erro ao extrair not√≠cia do artigo: {str(e)}")
            return None
    
    def _create_news_from_link(self, link) -> Optional[News]:
        """Cria uma not√≠cia a partir de um link relevante."""
        try:
            title = link.get_text(strip=True)
            if not title or len(title) < 10:
                return None
            
            href = link.get('href', '')
            if href.startswith('/'):
                url = urljoin(self.base_url, href)
            else:
                url = href
            
            return News(
                title=title,
                content=title,  # Usar t√≠tulo como conte√∫do inicial
                url=url,
                source="BBC News",
                published_date=datetime.now(),
                author=None
            )
            
        except Exception as e:
            print(f"Erro ao criar not√≠cia do link: {str(e)}")
            return None
    
    def _is_relevant_link(self, text: str, href: str) -> bool:
        """Verifica se um link √© relevante para Adam Sandler."""
        text_lower = text.lower()
        href_lower = href.lower()
        
        keywords = ["adam sandler", "sandler", "comedy", "netflix", "movie", "film"]
        
        return any(keyword in text_lower or keyword in href_lower for keyword in keywords)
    
    async def get_article_content(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """Busca o conte√∫do completo de um artigo."""
        try:
            async with session.get(url, headers=self.headers, timeout=30) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # Seletores para conte√∫do do artigo da BBC
                content_selectors = [
                    '[data-component="text-block"]',
                    '.ssrcss-11r1m41-RichTextComponentWrapper',
                    'div[data-component="text-block"] p',
                    '.story-body__inner p'
                ]
                
                content_parts = []
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        for elem in elements:
                            text = elem.get_text(strip=True)
                            if text and len(text) > 20:
                                content_parts.append(text)
                        break
                
                return ' '.join(content_parts) if content_parts else None
                
        except Exception as e:
            print(f"Erro ao buscar conte√∫do do artigo: {str(e)}")
            return None